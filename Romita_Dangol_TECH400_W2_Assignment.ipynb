{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "w0udrJg-e8ev",
        "outputId": "faf2e26a-c28f-483f-afeb-2e11c09898dc"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n",
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n"
          ]
        }
      ],
      "source": [
        "import nltk\n",
        "nltk.download('stopwords')\n",
        "nltk.download('punkt')\n",
        "nltk.download('wordnet')\n",
        "import os\n",
        "import string\n",
        "import logging\n",
        "import re # Import regular expressions library\n",
        "from collections import defaultdict , Counter\n",
        "from nltk . corpus import stopwords\n",
        "from nltk . tokenize import word_tokenize\n",
        "from nltk . stem import WordNetLemmatizer\n",
        "# Initialize the stop words and lemmatizer\n",
        "STOPWORDS = set( stopwords . words ('english') )\n",
        "LEMMATIZER = WordNetLemmatizer ()"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Function to load documents from a specified directory\n",
        "def load_documents(directory):\n",
        "    documents = {}\n",
        "    for filename in os.listdir(directory):\n",
        "        if filename.endswith(\".txt\"):\n",
        "            with open(os.path.join(directory, filename), 'r') as file:\n",
        "                documents[filename] = file.read()\n",
        "    return documents\n",
        "\n",
        "documents = load_documents('directory')"
      ],
      "metadata": {
        "id": "VTHPpo6wfNqC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Function to clean and preprocess text (lowercase, tokenization, stopwords removal, and lemmatization)\n",
        "def clean_text(text):\n",
        "    text = text.lower()\n",
        "    text = re.sub(r'\\W+', ' ', text)\n",
        "    tokens = word_tokenize(text)\n",
        "    tokens = [LEMMATIZER.lemmatize(token) for token in tokens if token not in STOPWORDS]\n",
        "    return tokens\n",
        "\n",
        "cleaned_documents = {filename: clean_text(content) for filename, content in documents.items()}"
      ],
      "metadata": {
        "id": "cW222E7dfn_v"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Function to create an inverted index\n",
        "def create_inverted_index(documents):\n",
        "    inverted_index = defaultdict(set)\n",
        "    for filename, tokens in documents.items():\n",
        "        for word in tokens:\n",
        "            inverted_index[word].add(filename)\n",
        "    return inverted_index\n",
        "\n",
        "inverted_index = create_inverted_index(cleaned_documents)"
      ],
      "metadata": {
        "id": "aYa594LbftfU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Initialize all_documents with the set of all document filenames\n",
        "all_documents = set(documents.keys())\n",
        "\n",
        "# Function for 'AND' query (finds common documents for all terms)\n",
        "def and_query(terms, inverted_index):\n",
        "    result = inverted_index.get(terms[0], set())\n",
        "    for term in terms[1:]:\n",
        "        result &= inverted_index.get(term, set())\n",
        "    return result"
      ],
      "metadata": {
        "id": "enAZDPFbfwsa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Function for 'OR' query (finds documents that contain any of the terms)\n",
        "def or_query(terms, inverted_index):\n",
        "    result = inverted_index.get(terms[0], set())\n",
        "    for term in terms[1:]:\n",
        "        result |= inverted_index.get(term, set())\n",
        "    return result"
      ],
      "metadata": {
        "id": "8P79SN4Af0fa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Function for 'NOT' query (finds documents that do not contain the specified term)\n",
        "def not_query(term, inverted_index, all_documents):\n",
        "    return all_documents - inverted_index.get(term, set())"
      ],
      "metadata": {
        "id": "RIdifrrZf47q"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Function to convert document IDs (filenames) to a list\n",
        "def convert_doc_ids_to_filenames(doc_ids):\n",
        "    return list(doc_ids)"
      ],
      "metadata": {
        "id": "bYPiWgHff9I6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Function to process the query and execute the appropriate Boolean operation\n",
        "def process_query(query, inverted_index, all_documents):\n",
        "    # Tokenize and preprocess the query\n",
        "    terms = [LEMMATIZER.lemmatize(term) for term in word_tokenize(query.lower()) if term not in STOPWORDS]\n",
        "    if 'and' in terms:\n",
        "        terms.remove('and')\n",
        "        result = and_query(terms, inverted_index)\n",
        "    elif 'or' in terms:\n",
        "        terms.remove('or')\n",
        "        result = or_query(terms, inverted_index)\n",
        "    elif 'not' in terms:\n",
        "        terms.remove('not')\n",
        "        result = not_query(terms[0], inverted_index, all_documents)\n",
        "    else:\n",
        "        result = inverted_index.get(terms[0], set())\n",
        "    return convert_doc_ids_to_filenames(result)"
      ],
      "metadata": {
        "id": "Wm0xH5togABj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Example usage\n",
        "query = \"not see\"\n",
        "result = process_query(query, inverted_index, all_documents)\n",
        "print(result)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xh5OInvZgBO0",
        "outputId": "6f8cfa2a-4786-4692-81d3-3459aa4c611e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['Song 7.txt', 'Song 6.txt', 'Song 3.txt', 'Song 10.txt']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Example usage\n",
        "query = \"play and cool\"\n",
        "result = process_query(query, inverted_index, all_documents)\n",
        "print(result)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EG45A47U7qir",
        "outputId": "9e587284-fe80-481d-a15d-9256ba8130ee"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['Song 7.txt']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Example usage\n",
        "query = \"baby or know\"\n",
        "result = process_query(query, inverted_index, all_documents)\n",
        "print(result)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pZlcv4Oq72hM",
        "outputId": "c0a1b9c8-5564-4777-98e9-0bb52450257c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['Song 7.txt', 'Song 6.txt', 'Song 10.txt', 'Song 9.txt']\n"
          ]
        }
      ]
    }
  ]
}